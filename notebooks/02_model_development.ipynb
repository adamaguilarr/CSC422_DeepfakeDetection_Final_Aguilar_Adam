{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb38a34",
   "metadata": {},
   "source": [
    "# 02 – Model Development: Deepfake Detection\n",
    "\n",
    "Goal: extract image frames from videos, build a train/val split, and prepare data\n",
    "loaders for a CNN based deepfake classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830efe8",
   "metadata": {},
   "source": [
    "Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba5cb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85539717",
   "metadata": {},
   "source": [
    "Load metadata again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0d401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamc\\Documents\\Fall 25\\Machine and Deep Learning\\CSC422_DeepfakeDetection_Final_Aguilar_Adam\\data\\raw\\train_sample_videos\\metadata.json True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(         filename label        original\n",
       " 0  aagfhgtpmv.mp4  FAKE  vudstovrck.mp4\n",
       " 1  aapnvogymq.mp4  FAKE  jdubbvfswz.mp4\n",
       " 2  abarnvbtwb.mp4  REAL            None\n",
       " 3  abofeumbvv.mp4  FAKE  atvmxvwyns.mp4\n",
       " 4  abqwwspghj.mp4  FAKE  qzimuostzz.mp4,\n",
       " label\n",
       " FAKE    323\n",
       " REAL     77\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# project root is one level up from notebooks/\n",
    "project_root = Path(\"..\").resolve()\n",
    "\n",
    "# data/raw under project root\n",
    "data_root = project_root / \"data\" / \"raw\"\n",
    "\n",
    "meta_path = data_root / \"train_sample_videos\" / \"metadata.json\"\n",
    "print(meta_path, meta_path.exists())\n",
    "\n",
    "with open(meta_path, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "rows = []\n",
    "for fname, info in meta.items():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"filename\": fname,\n",
    "            \"label\": info[\"label\"],          # \"FAKE\" or \"REAL\"\n",
    "            \"original\": info.get(\"original\") # original real video, if available\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.head(), df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40951c11",
   "metadata": {},
   "source": [
    "Create processed frame folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d67ab78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/processed/frames/train')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_root = Path(\"data/processed/frames\")\n",
    "train_frames_root = frames_root / \"train\"\n",
    "\n",
    "for label in [\"REAL\", \"FAKE\"]:\n",
    "    (train_frames_root / label).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_frames_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c51dda",
   "metadata": {},
   "source": [
    "Helper to extract a center frame from each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9de453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_center_frame(video_path: Path, out_path: Path) -> bool:\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(\"Could not open\", video_path)\n",
    "        return False\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_count == 0:\n",
    "        cap.release()\n",
    "        print(\"No frames in\", video_path)\n",
    "        return False\n",
    "\n",
    "    center_idx = frame_count // 2\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, center_idx)\n",
    "    ok, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ok:\n",
    "        print(\"Could not read frame from\", video_path)\n",
    "        return False\n",
    "\n",
    "    # OpenCV is BGR, but for saving jpg it is fine to keep BGR\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cv2.imwrite(str(out_path), frame)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b3e11",
   "metadata": {},
   "source": [
    "Run frame extraction (one per video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67fee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adamc\\Documents\\Fall 25\\Machine and Deep Learning\\CSC422_DeepfakeDetection_Final_Aguilar_Adam\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 400/400 [01:49<00:00,  3.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "success = 0\n",
    "fail = 0\n",
    "\n",
    "for row in tqdm(df.itertuples(), total=len(df)):\n",
    "    fname = row.filename\n",
    "    label = row.label.upper()          # \"REAL\" or \"FAKE\"\n",
    "\n",
    "    video_path = data_root / \"train_sample_videos\" / fname\n",
    "    out_name = fname.replace(\".mp4\", \".jpg\")\n",
    "    out_path = train_frames_root / label / out_name\n",
    "\n",
    "    if out_path.exists():\n",
    "        continue\n",
    "\n",
    "    if extract_center_frame(video_path, out_path):\n",
    "        success += 1\n",
    "    else:\n",
    "        fail += 1\n",
    "\n",
    "success, fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45035985",
   "metadata": {},
   "source": [
    "Filter to videos that actually have frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6151b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,\n",
       " 400,\n",
       " label\n",
       " FAKE    323\n",
       " REAL     77\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# we already used these earlier\n",
    "frames_root = Path(\"data/processed/frames\")\n",
    "train_frames_root = frames_root / \"train\"\n",
    "\n",
    "def has_extracted_frame(row):\n",
    "    label_str = row[\"label\"].upper()\n",
    "    fname = row[\"filename\"].replace(\".mp4\", \".jpg\")\n",
    "    img_path = train_frames_root / label_str / fname\n",
    "    return img_path.exists()\n",
    "\n",
    "df_frames = df[df.apply(has_extracted_frame, axis=1)].reset_index(drop=True)\n",
    "len(df), len(df_frames), df_frames[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0bfc7",
   "metadata": {},
   "source": [
    "Create a dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2715d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, root, df, transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.label_map = {\"REAL\": 0, \"FAKE\": 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        label_str = row[\"label\"].upper()\n",
    "        label = self.label_map[label_str]\n",
    "\n",
    "        fname = row[\"filename\"].replace(\".mp4\", \".jpg\")\n",
    "        img_path = self.root / label_str / fname\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d917c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 80)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df_frames,\n",
    "    test_size=0.2,\n",
    "    stratify=df_frames[\"label\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((image_size, image_size)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((image_size, image_size)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = FrameDataset(train_frames_root, train_df, transform=train_transform)\n",
    "val_ds   = FrameDataset(train_frames_root, val_df,   transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3c140",
   "metadata": {},
   "source": [
    "Define and train a small model (this uses ResNet18 with 2 output classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bf480e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\adamc/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:02<00:00, 21.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c7ff0",
   "metadata": {},
   "source": [
    "Training loop (keep epochs small so it doesnt take forever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ebb42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.7157  acc: 0.594\n",
      "  Val   loss: 0.4969  acc: 0.800\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.4834  acc: 0.787\n",
      "  Val   loss: 0.5102  acc: 0.750\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.4033  acc: 0.812\n",
      "  Val   loss: 0.5025  acc: 0.800\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.3592  acc: 0.831\n",
      "  Val   loss: 0.5418  acc: 0.775\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.3821  acc: 0.825\n",
      "  Val   loss: 0.4773  acc: 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for images, labels in tqdm(loader, leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "num_epochs = 5  # you can bump to 8–10 if it’s still fast enough\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_acc     = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(f\"  Train loss: {train_loss:.4f}  acc: {train_acc:.3f}\")\n",
    "    print(f\"  Val   loss: {val_loss:.4f}  acc: {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d5468",
   "metadata": {},
   "source": [
    "Save the trained model for notebook 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f13dd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: C:\\Users\\adamc\\Documents\\Fall 25\\Machine and Deep Learning\\CSC422_DeepfakeDetection_Final_Aguilar_Adam\\models\\deepfake_resnet18.pth\n"
     ]
    }
   ],
   "source": [
    "# Save trained model weights for later evaluation\n",
    "project_root = Path(\"..\").resolve()\n",
    "models_dir = project_root / \"models\"\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = models_dir / \"deepfake_resnet18.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Saved model to:\", model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
